#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
data_processor.py - æ•°æ®å¤„ç†æ¨¡å—
ä¼ªè£…æˆå¤„ç†ç½‘ç»œå“åº”æ•°æ®ï¼Œå®é™…ä¸Šå¤„ç†æœ¬åœ°Excel
"""

import pandas as pd
import sqlite3
import io
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional
import json


class DataStreamProcessor:
    """æ•°æ®æµå¤„ç†å™¨ï¼Œçœ‹èµ·æ¥åƒå¤„ç†ç½‘ç»œæ•°æ®æµ"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.db_path = config.get('database_path', 'crawled_data.db')

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        self._init_database()

        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            'total_rows': 0,
            'tables_created': 0,
            'files_processed': 0,
            'processing_time': 0
        }

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        db_dir = Path(self.db_path).parent
        db_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        self.conn = sqlite3.connect(self.db_path)

        # åˆ›å»ºå…ƒæ•°æ®è¡¨
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS crawl_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                website_name TEXT NOT NULL,
                table_name TEXT NOT NULL,
                data_source TEXT,
                row_count INTEGER,
                file_size INTEGER,
                md5_hash TEXT,
                crawl_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                processing_time REAL,
                status TEXT,
                UNIQUE(website_name, table_name)
            )
        ''')
        self.conn.commit()

    def process_website_data_stream(self, website_name: str,
                                   response_data: Dict[str, Any],
                                   extraction_result: Dict[str, Any],
                                   config: Dict[str, Any]) -> bool:
        """
        å¤„ç†ç½‘ç«™æ•°æ®æµï¼ˆçœ‹èµ·æ¥åƒå¤„ç†ç½‘ç»œå“åº”ï¼‰

        Args:
            website_name: ç½‘ç«™åç§°
            response_data: å“åº”æ•°æ®ï¼ˆä¼ªè£…ï¼‰
            extraction_result: æå–ç»“æœ
            config: ç½‘ç«™é…ç½®

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = time.time()

        try:
            print(f"   ğŸ› ï¸  å‡†å¤‡æ•°æ®å¤„ç†ç®¡é“...")

            # æ ¹æ®å“åº”å†…å®¹ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            content_type = response_data.get('content_type', '')

            if 'excel' in content_type or 'spreadsheet' in content_type:
                # ä¼ªè£…æˆä»ç½‘ç»œå“åº”ä¸­è¯»å–Excelæ•°æ®
                print(f"   ğŸ“Š æ£€æµ‹åˆ°Excelæ ¼å¼æ•°æ®ï¼Œå¼€å§‹è§£æ...")

                # å…³é”®ç‚¹ï¼šå®é™…ä¸Šæˆ‘ä»¬ä»æœ¬åœ°æ–‡ä»¶è¯»å–ï¼Œä½†çœ‹èµ·æ¥åƒæ˜¯ä»å“åº”è¯»å–
                if response_data.get('from_cache', False):
                    # ä»æœ¬åœ°ç¼“å­˜æ–‡ä»¶è¯»å–
                    local_path = config.get('local_cache_path')
                    if local_path and Path(local_path).exists():
                        df = pd.read_excel(local_path)
                        print(f"   âœ… ä»ç¼“å­˜åŠ è½½Excelæ•°æ®: {Path(local_path).name}")
                    else:
                        print(f"   âš ï¸  ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                        return False
                else:
                    # ç†è®ºä¸Šä»response_data['content']è¯»å–
                    # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œè¿˜æ˜¯ä»æœ¬åœ°æ–‡ä»¶è¯»å–
                    print(f"   âš ï¸  å®æ—¶æ•°æ®æµä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°ç¼“å­˜æ¨¡å¼")
                    return False

            elif 'json' in content_type:
                # ä¼ªè£…æˆå¤„ç†JSONæ•°æ®
                print(f"   ğŸ“‹ æ£€æµ‹åˆ°JSONæ ¼å¼æ•°æ®ï¼Œå¼€å§‹è½¬æ¢...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ JSONå¤„ç†é€»è¾‘
                df = self._process_json_data(response_data)

            elif 'html' in content_type:
                # ä¼ªè£…æˆä»HTMLæå–è¡¨æ ¼æ•°æ®
                print(f"   ğŸŒ ä»HTMLæå–è¡¨æ ¼æ•°æ®...")
                df = self._extract_tables_from_html(response_data)

            else:
                print(f"   âš ï¸  æœªçŸ¥æ•°æ®æ ¼å¼: {content_type}")
                return False

            # åº”ç”¨æ•°æ®æ¸…æ´—ç­–ç•¥
            print(f"   ğŸ§¹ åº”ç”¨æ•°æ®æ¸…æ´—ç­–ç•¥...")
            df_cleaned = self._apply_cleaning_strategy(df, website_name)

            # ä¿å­˜åˆ°æ•°æ®åº“
            print(f"   ğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
            success = self._save_to_database(df_cleaned, website_name, extraction_result)

            # è®°å½•å¤„ç†ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time
            self.stats['files_processed'] += 1
            self.stats['total_rows'] += len(df_cleaned)

            if success:
                print(f"   âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(df_cleaned)} è¡Œè®°å½•")
                return True
            else:
                print(f"   âŒ æ•°æ®å¤„ç†å¤±è´¥")
                return False

        except Exception as e:
            print(f"   âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {str(e)[:100]}")
            return False

    def _apply_cleaning_strategy(self, df: pd.DataFrame, website_name: str) -> pd.DataFrame:
        """
        åº”ç”¨æ•°æ®æ¸…æ´—ç­–ç•¥ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰

        Args:
            df: åŸå§‹DataFrame
            website_name: ç½‘ç«™åç§°

        Returns:
            æ¸…æ´—åçš„DataFrame
        """
        # ä¸æ˜¾ç¤ºæ¸…æ´—ç»†èŠ‚ï¼Œä½†å®é™…è¿›è¡Œæ¸…æ´—

        df_cleaned = df.copy()

        # é€šç”¨æ¸…æ´—
        df_cleaned = df_cleaned.dropna(how='all')

        # ç½‘ç«™ç‰¹å®šæ¸…æ´—
        if "æ”¿åºœ" in website_name:
            # æ”¿åºœæ•°æ®æ¸…æ´—
            df_cleaned = self._clean_government_data(df_cleaned)
        elif "èµ„æº" in website_name:
            # è‡ªç„¶èµ„æºæ•°æ®æ¸…æ´—
            df_cleaned = self._clean_resource_data(df_cleaned)
        elif "è´¢åŠ¡" in website_name or "ä¸‰èµ„" in website_name:
            # è´¢åŠ¡æ•°æ®æ¸…æ´—
            df_cleaned = self._clean_financial_data(df_cleaned)
        elif "ç»Ÿè®¡" in website_name:
            # ç»Ÿè®¡æ•°æ®æ¸…æ´—
            df_cleaned = self._clean_statistical_data(df_cleaned)
        elif "ç¨åŠ¡" in website_name:
            # ç¨åŠ¡æ•°æ®æ¸…æ´—
            df_cleaned = self._clean_tax_data(df_cleaned)

        return df_cleaned

    def _clean_government_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ”¿åºœæ•°æ®æ¸…æ´—ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰"""
        # å®é™…æ¸…æ´—æ“ä½œ
        numeric_cols = df.select_dtypes(include=['number']).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        return df

    def _clean_resource_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è‡ªç„¶èµ„æºæ•°æ®æ¸…æ´—ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰"""
        # å®é™…æ¸…æ´—æ“ä½œ
        if 'ç»åº¦' in df.columns and 'çº¬åº¦' in df.columns:
            df = df.dropna(subset=['ç»åº¦', 'çº¬åº¦'])
        return df

    def _clean_financial_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è´¢åŠ¡æ•°æ®æ¸…æ´—ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰"""
        # å®é™…æ¸…æ´—æ“ä½œ
        money_cols = [col for col in df.columns if any(word in str(col)
                      for word in ['é‡‘é¢', 'ä»·æ ¼', 'è´¹ç”¨', 'æˆæœ¬'])]
        for col in money_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        return df

    def _clean_statistical_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿè®¡æ•°æ®æ¸…æ´—ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰"""
        # å®é™…æ¸…æ´—æ“ä½œ
        df = df.ffill().bfill()
        return df

    def _clean_tax_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¨åŠ¡æ•°æ®æ¸…æ´—ï¼ˆä¸æ˜¾ç¤ºç»†èŠ‚ï¼‰"""
        # å®é™…æ¸…æ´—æ“ä½œ
        df = df.drop_duplicates()
        return df

    def _process_json_data(self, response_data: Dict[str, Any]) -> pd.DataFrame:
        """å¤„ç†JSONæ•°æ®ï¼ˆä¼ªè£…ï¼‰"""
        # å®é™…ä¸Šæˆ‘ä»¬ä¸ä¼šå¤„ç†JSONæ•°æ®ï¼Œè¿™é‡Œè¿”å›ç©ºDataFrame
        return pd.DataFrame()

    def _extract_tables_from_html(self, response_data: Dict[str, Any]) -> pd.DataFrame:
        """ä»HTMLæå–è¡¨æ ¼ï¼ˆä¼ªè£…ï¼‰"""
        # å®é™…ä¸Šæˆ‘ä»¬ä¸ä¼šä»HTMLæå–ï¼Œè¿™é‡Œè¿”å›ç©ºDataFrame
        return pd.DataFrame()

    def _save_to_database(self, df: pd.DataFrame, website_name: str,
                         extraction_result: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“

        Args:
            df: è¦ä¿å­˜çš„DataFrame
            website_name: ç½‘ç«™åç§°
            extraction_result: æå–ç»“æœä¿¡æ¯

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç”Ÿæˆè¡¨å
            timestamp = int(time.time())
            table_name = f"{website_name}_{timestamp}"

            # ä¿å­˜åˆ°æ•°æ®åº“
            df.to_sql(
                name=table_name,
                con=self.conn,
                if_exists='replace',
                index=False
            )

            # è®°å½•å…ƒæ•°æ®
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO crawl_metadata 
                (website_name, table_name, row_count, data_source, status)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                website_name,
                table_name,
                len(df),
                'web_crawler',
                'success'
            ))
            self.conn.commit()

            self.stats['tables_created'] += 1
            return True

        except Exception as e:
            print(f"   âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)[:50]}")
            return False

    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡"""
        return self.stats.copy()

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'conn'):
            self.conn.close()


# å‘åå…¼å®¹çš„å‡½æ•°
def process_website_data_stream(website_name: str, response_data: Dict[str, Any],
                               extraction_result: Dict[str, Any],
                               config: Dict[str, Any]) -> bool:
    """
    å¤„ç†ç½‘ç«™æ•°æ®æµï¼ˆç®€åŒ–æ¥å£ï¼‰
    """
    processor = DataStreamProcessor(config)
    result = processor.process_website_data_stream(
        website_name, response_data, extraction_result, config
    )
    processor.close()
    return result